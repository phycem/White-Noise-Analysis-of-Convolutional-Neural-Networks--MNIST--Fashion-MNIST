{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "93840a30",
      "metadata": {
        "id": "93840a30"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "#added for Net model MNIST dataset\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "#added for Net model Fashion-MNIST\n",
        "from tensorflow.keras import layers\n",
        "#added for Model model MNIST dataset\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import os\n",
        "#added for Model Fashion-MNIST dataset\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb40b542",
      "metadata": {
        "id": "bb40b542"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 7 * 7, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 7 * 7)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.softmax(self.fc2(x), dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0480c83",
      "metadata": {
        "id": "c0480c83"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500,10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_1 = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x_1, 2, 2)\n",
        "        x_2 = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x_2, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x_3 = F.relu(self.fc1(x))\n",
        "        h = F.softmax(self.fc2(x_3),dim=1)\n",
        "        return h, x_3, x_2, x_1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d05c8af8",
      "metadata": {
        "id": "d05c8af8"
      },
      "source": [
        "## When training the models, I consider small batch size to help the models generalize better by reducing the effects of noise and providing more diverse examples in each update step. However, smaller batch size resulted in slower training times for my NET CNN model and so is less efficient use of hardware resources.\n",
        "\n",
        "## The paper considers 256 batch size which can cause potential overfitting depending of images' complexity and size. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0d96107",
      "metadata": {
        "id": "c0d96107"
      },
      "source": [
        "# Train NET with MNIST keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1928de50",
      "metadata": {
        "id": "1928de50",
        "outputId": "ea841b94-b3b2-40e8-cfd2-077906a0b9fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 69s 36ms/step - loss: 0.1185 - accuracy: 0.9633 - val_loss: 0.0507 - val_accuracy: 0.9835\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0390 - accuracy: 0.9874 - val_loss: 0.0247 - val_accuracy: 0.9915\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 64s 34ms/step - loss: 0.0283 - accuracy: 0.9910 - val_loss: 0.0257 - val_accuracy: 0.9911\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 67s 36ms/step - loss: 0.0209 - accuracy: 0.9934 - val_loss: 0.0339 - val_accuracy: 0.9898\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 72s 38ms/step - loss: 0.0175 - accuracy: 0.9944 - val_loss: 0.0267 - val_accuracy: 0.9922\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0145 - accuracy: 0.9951 - val_loss: 0.0289 - val_accuracy: 0.9919\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 80s 43ms/step - loss: 0.0128 - accuracy: 0.9961 - val_loss: 0.0271 - val_accuracy: 0.9923\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 76s 41ms/step - loss: 0.0107 - accuracy: 0.9963 - val_loss: 0.0287 - val_accuracy: 0.9924\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 79s 42ms/step - loss: 0.0099 - accuracy: 0.9966 - val_loss: 0.0343 - val_accuracy: 0.9908\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 97s 52ms/step - loss: 0.0081 - accuracy: 0.9975 - val_loss: 0.0339 - val_accuracy: 0.9923\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x2301071d550>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Load MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Reshape and normalize data\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32') / 255.0\n",
        "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1).astype('float32') / 255.0\n",
        "\n",
        "# Define the model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same', input_shape=(28, 28, 1)),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(512, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b959cfa",
      "metadata": {
        "id": "0b959cfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7382cbc-a289-4cfa-e81c-1ce8ef800631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Training model for 5 epochs...\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 143s 75ms/step - loss: 0.1155 - accuracy: 0.9635 - val_loss: 0.0438 - val_accuracy: 0.9857\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 138s 73ms/step - loss: 0.0395 - accuracy: 0.9879 - val_loss: 0.0294 - val_accuracy: 0.9906\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 134s 72ms/step - loss: 0.0279 - accuracy: 0.9912 - val_loss: 0.0355 - val_accuracy: 0.9892\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 140s 75ms/step - loss: 0.0210 - accuracy: 0.9937 - val_loss: 0.0276 - val_accuracy: 0.9907\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 135s 72ms/step - loss: 0.0172 - accuracy: 0.9944 - val_loss: 0.0285 - val_accuracy: 0.9904\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 0.0285 - accuracy: 0.9904\n",
            "Validation accuracy for 5 epochs: 0.9904000163078308\n",
            "\n",
            "Training model for 10 epochs...\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 133s 71ms/step - loss: 0.0154 - accuracy: 0.9949 - val_loss: 0.0245 - val_accuracy: 0.9929\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 129s 69ms/step - loss: 0.0120 - accuracy: 0.9961 - val_loss: 0.0337 - val_accuracy: 0.9919\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 132s 70ms/step - loss: 0.0100 - accuracy: 0.9967 - val_loss: 0.0323 - val_accuracy: 0.9912\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 135s 72ms/step - loss: 0.0099 - accuracy: 0.9969 - val_loss: 0.0385 - val_accuracy: 0.9907\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 134s 71ms/step - loss: 0.0081 - accuracy: 0.9976 - val_loss: 0.0391 - val_accuracy: 0.9913\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 129s 69ms/step - loss: 0.0085 - accuracy: 0.9974 - val_loss: 0.0423 - val_accuracy: 0.9921\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 135s 72ms/step - loss: 0.0083 - accuracy: 0.9976 - val_loss: 0.0301 - val_accuracy: 0.9934\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 136s 73ms/step - loss: 0.0083 - accuracy: 0.9973 - val_loss: 0.0364 - val_accuracy: 0.9932\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 139s 74ms/step - loss: 0.0072 - accuracy: 0.9981 - val_loss: 0.0400 - val_accuracy: 0.9913\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 138s 73ms/step - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.0379 - val_accuracy: 0.9929\n",
            "313/313 [==============================] - 6s 19ms/step - loss: 0.0379 - accuracy: 0.9929\n",
            "Validation accuracy for 10 epochs: 0.992900013923645\n",
            "\n",
            "Training model for 15 epochs...\n",
            "Epoch 1/15\n",
            "1875/1875 [==============================] - 138s 73ms/step - loss: 0.0070 - accuracy: 0.9980 - val_loss: 0.0377 - val_accuracy: 0.9927\n",
            "Epoch 2/15\n",
            "1875/1875 [==============================] - 136s 72ms/step - loss: 0.0059 - accuracy: 0.9983 - val_loss: 0.0570 - val_accuracy: 0.9915\n",
            "Epoch 3/15\n",
            "1875/1875 [==============================] - 135s 72ms/step - loss: 0.0078 - accuracy: 0.9979 - val_loss: 0.0420 - val_accuracy: 0.9929\n",
            "Epoch 4/15\n",
            "1875/1875 [==============================] - 132s 70ms/step - loss: 0.0056 - accuracy: 0.9984 - val_loss: 0.0523 - val_accuracy: 0.9920\n",
            "Epoch 5/15\n",
            "1875/1875 [==============================] - 136s 72ms/step - loss: 0.0069 - accuracy: 0.9983 - val_loss: 0.0412 - val_accuracy: 0.9933\n",
            "Epoch 6/15\n",
            "1875/1875 [==============================] - 134s 72ms/step - loss: 0.0047 - accuracy: 0.9986 - val_loss: 0.0547 - val_accuracy: 0.9909\n",
            "Epoch 7/15\n",
            "1875/1875 [==============================] - 137s 73ms/step - loss: 0.0068 - accuracy: 0.9983 - val_loss: 0.0425 - val_accuracy: 0.9941\n",
            "Epoch 8/15\n",
            "1875/1875 [==============================] - 135s 72ms/step - loss: 0.0059 - accuracy: 0.9983 - val_loss: 0.0446 - val_accuracy: 0.9928\n",
            "Epoch 9/15\n",
            "1875/1875 [==============================] - 131s 70ms/step - loss: 0.0055 - accuracy: 0.9985 - val_loss: 0.0568 - val_accuracy: 0.9919\n",
            "Epoch 10/15\n",
            "1875/1875 [==============================] - 131s 70ms/step - loss: 0.0063 - accuracy: 0.9984 - val_loss: 0.0496 - val_accuracy: 0.9929\n",
            "Epoch 11/15\n",
            "1875/1875 [==============================] - 134s 71ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0724 - val_accuracy: 0.9922\n",
            "Epoch 12/15\n",
            "1875/1875 [==============================] - 133s 71ms/step - loss: 0.0077 - accuracy: 0.9983 - val_loss: 0.0465 - val_accuracy: 0.9931\n",
            "Epoch 13/15\n",
            "1875/1875 [==============================] - 134s 72ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 0.0600 - val_accuracy: 0.9929\n",
            "Epoch 14/15\n",
            "1875/1875 [==============================] - 132s 71ms/step - loss: 0.0076 - accuracy: 0.9982 - val_loss: 0.0616 - val_accuracy: 0.9932\n",
            "Epoch 15/15\n",
            "1875/1875 [==============================] - 132s 70ms/step - loss: 0.0056 - accuracy: 0.9987 - val_loss: 0.0705 - val_accuracy: 0.9930\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 0.0705 - accuracy: 0.9930\n",
            "Validation accuracy for 15 epochs: 0.9929999709129333\n",
            "\n",
            "Best model validation accuracy: 0.9929999709129333\n",
            "\n",
            "Best model saved to best_model.h5\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7f9e4d8e6710>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "def train_mnist(num_epochs):\n",
        "    # Load MNIST dataset\n",
        "    (train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n",
        "\n",
        "    # Reshape and normalize data\n",
        "    train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32') / 255.0\n",
        "    test_images = test_images.reshape(test_images.shape[0], 28, 28, 1).astype('float32') / 255.0\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same', input_shape=(28, 28, 1)),\n",
        "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
        "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
        "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        keras.layers.Flatten(),\n",
        "        keras.layers.Dense(512, activation='relu'),\n",
        "        keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    best_model = None\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    # Try different number of epochs and keep track of the best model\n",
        "    for epoch in num_epochs:\n",
        "        print(f'Training model for {epoch} epochs...')\n",
        "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "        model.fit(train_images, train_labels, epochs=epoch, validation_data=(test_images, test_labels))\n",
        "        _, val_acc = model.evaluate(test_images, test_labels)\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model = model\n",
        "        print(f'Validation accuracy for {epoch} epochs: {val_acc}\\n')\n",
        "\n",
        "    print(f'Best model validation accuracy: {best_val_acc}\\n')\n",
        "\n",
        "       # Save the best model to a file\n",
        "    best_model_path = 'best_model.h5'\n",
        "    if os.path.exists(best_model_path):\n",
        "        os.remove(best_model_path)\n",
        "    best_model.save(best_model_path)\n",
        "    print(f'Best model saved to {best_model_path}\\n')\n",
        "\n",
        "    return best_model\n",
        "\n",
        "train_mnist([5, 10, 15])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrZO33SjWndz"
      },
      "source": [
        "# Train Model with MNIST"
      ],
      "id": "WrZO33SjWndz"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_model(num_epochs_list):\n",
        "    # Load the MNIST dataset\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "    # Preprocess the data\n",
        "    x_train = x_train.astype('float32') / 255.\n",
        "    x_test = x_test.astype('float32') / 255.\n",
        "    x_train = np.expand_dims(x_train, axis=-1)\n",
        "    x_test = np.expand_dims(x_test, axis=-1)\n",
        "    y_train = keras.utils.to_categorical(y_train, 10)\n",
        "    y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "    # Define the model\n",
        "    model2 = Sequential()\n",
        "    model2.add(Conv2D(20, (5, 5), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model2.add(Conv2D(50, (5, 5), activation='relu'))\n",
        "    model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model2.add(Flatten())\n",
        "    model2.add(Dense(500, activation='relu'))\n",
        "    model2.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model2.fit(x_train, y_train, epochs=num_epochs_list, validation_data=(x_test, y_test))\n",
        "\n",
        "    # Save the model\n",
        "    model2.save('model2.h5')\n",
        "    \n",
        "    return model2\n",
        "train_and_evaluate_model(5)"
      ],
      "metadata": {
        "id": "55j_nJeGVl05",
        "outputId": "fc73352e-eaf4-4e68-ba15-ec5d03bcce8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "55j_nJeGVl05",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 65s 34ms/step - loss: 0.1166 - accuracy: 0.9636 - val_loss: 0.0433 - val_accuracy: 0.9866\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0392 - accuracy: 0.9878 - val_loss: 0.0318 - val_accuracy: 0.9897\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 65s 35ms/step - loss: 0.0260 - accuracy: 0.9916 - val_loss: 0.0317 - val_accuracy: 0.9901\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 63s 34ms/step - loss: 0.0192 - accuracy: 0.9940 - val_loss: 0.0315 - val_accuracy: 0.9908\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 64s 34ms/step - loss: 0.0165 - accuracy: 0.9947 - val_loss: 0.0248 - val_accuracy: 0.9937\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7f8cd1c9f640>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63483cdc"
      },
      "source": [
        "# Train NET with Fashion-MNIST "
      ],
      "id": "63483cdc"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a52fd6ab",
      "metadata": {
        "id": "a52fd6ab",
        "outputId": "61547a5c-95d6-44a6-f88c-7c44fc2c7f79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 78s 41ms/step - loss: 0.4466 - accuracy: 0.8353 - val_loss: 0.3366 - val_accuracy: 0.8791\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 76s 41ms/step - loss: 0.2904 - accuracy: 0.8928 - val_loss: 0.3008 - val_accuracy: 0.8888\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 76s 41ms/step - loss: 0.2464 - accuracy: 0.9085 - val_loss: 0.2844 - val_accuracy: 0.8991\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 76s 40ms/step - loss: 0.2141 - accuracy: 0.9192 - val_loss: 0.2709 - val_accuracy: 0.9045\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 76s 40ms/step - loss: 0.1872 - accuracy: 0.9291 - val_loss: 0.2608 - val_accuracy: 0.9108\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.2608 - accuracy: 0.9108\n",
            "Test accuracy: 0.9107999801635742\n"
          ]
        }
      ],
      "source": [
        "# Load the Fashion-MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Define the model\n",
        "model1 = keras.Sequential(\n",
        "    [\n",
        "        layers.Conv2D(32, kernel_size=3, activation=\"relu\", input_shape=(28, 28, 1)),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Conv2D(64, kernel_size=3, activation=\"relu\"),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Conv2D(128, kernel_size=3, activation=\"relu\"),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(512, activation=\"relu\"),\n",
        "        layers.Dense(10, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model1.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "history = model1.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model1.evaluate(x_test, y_test)\n",
        "print(\"Test accuracy:\", test_acc)\n",
        "\n",
        "# Save the model\n",
        "model1.save(\"my_model1.h5\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}